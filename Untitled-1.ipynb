{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa56208e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sc-20250604-180901-4z0xlx Dataproc Session is not active, stopping and creating a new one\n",
      "Creating Dataproc Session: https://console.cloud.google.com/dataproc/interactive/us-central1/sc-20250604-201328-p9xooo?project=wortz-project-352116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "█████▎                                                                          "
     ]
    }
   ],
   "source": [
    "# %pip install dataproc-spark-connect\n",
    "from google.cloud.dataproc_v1 import SparkConnectConfig\n",
    "\n",
    "from google.cloud.dataproc_v1 import Session\n",
    "session_config = Session()\n",
    "session_config.session_template = 'projects/wortz-project-352116/locations/us-central1/sessionTemplates/spark-connect-2-2'\n",
    "session_config.spark_connect_session = SparkConnectConfig()\n",
    "session_config.runtime_config.version = '2.2'\n",
    "from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "spark = DataprocSparkSession.builder.projectId(\"wortz-project-352116\").location(\"us-central1\").dataprocSessionConfig(session_config).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeedb1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pipeline_params = dict(\n",
    "    project_id=\"wortz-project-352116\",\n",
    "    model=\"3416616934593003520\",\n",
    "    region=\"us-central1\",\n",
    "    zone=\"us-central1-b\",\n",
    "    reservation_name=\"a100-custom-image-reservation\",\n",
    "    endpoint_display_name=\"Reservation_Endpoint\",\n",
    "    deployed_model_display_name=\"My_deployed_model\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_A100\",\n",
    "    machine_type=\"a2-highgpu-1g\",\n",
    "    bucket=\"model_experimentation_2025\",\n",
    "    prediction_input_blob=\"prediction_data/test.csv\",\n",
    "    # prediction_output_blob=f\"output_data/predictions_{epoch_time}.jsonl\",\n",
    "    dataproc_serverless_template=\"projects/wortz-project-352116/locations/us-central1/sessionTemplates/runtime-0000da551e61\",\n",
    "    batch_size=30,\n",
    "    accelerator_count=1,\n",
    "    min_replica=1,\n",
    "    max_replica=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76f28b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import pandas_udf\n",
    "import pandas as pd\n",
    "from pyspark.sql.connect.types import FloatType, ArrayType\n",
    "import logging\n",
    "import pyspark\n",
    "import pyspark.sql.connect.functions as F\n",
    "from google.cloud.dataproc_v1 import Session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a704ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "google.cloud.dataproc_spark_connect.session.DataprocSparkSession"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96ab9041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749078664.645004 31885974 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = spark.read.option(\"header\", True).csv(f\"gs://{pipeline_params['bucket']}/{pipeline_params['prediction_input_blob']}\")\n",
    "\n",
    "prediction_data = df.drop(\"label\")\n",
    "\n",
    "cols = prediction_data.columns\n",
    "\n",
    "# prediction_data.createOrReplaceTempView(\"raw_prediction_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "04fbe5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_formatted_data = prediction_data.withColumn(\n",
    "        \"data\", F.array(*cols)\n",
    "    ).drop(*cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31835659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Session is no longer active\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Session not active. Please create a new session",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mpredictions_formatted_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:549\u001b[39m, in \u001b[36mDataFrame.take\u001b[39m\u001b[34m(self, num)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtake\u001b[39m(\u001b[38;5;28mself\u001b[39m, num: \u001b[38;5;28mint\u001b[39m) -> List[Row]:\n\u001b[32m--> \u001b[39m\u001b[32m549\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/connect/dataframe.py:1645\u001b[39m, in \u001b[36mDataFrame.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1643\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCannot collect on empty session.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1644\u001b[39m query = \u001b[38;5;28mself\u001b[39m._plan.to_proto(\u001b[38;5;28mself\u001b[39m._session.client)\n\u001b[32m-> \u001b[39m\u001b[32m1645\u001b[39m table, schema = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1647\u001b[39m schema = schema \u001b[38;5;129;01mor\u001b[39;00m from_arrow_schema(table.schema, prefer_timestamp_ntz=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1649\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, StructType)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:858\u001b[39m, in \u001b[36mSparkConnectClient.to_table\u001b[39m\u001b[34m(self, plan)\u001b[39m\n\u001b[32m    856\u001b[39m req = \u001b[38;5;28mself\u001b[39m._execute_plan_request_with_metadata()\n\u001b[32m    857\u001b[39m req.plan.CopyFrom(plan)\n\u001b[32m--> \u001b[39m\u001b[32m858\u001b[39m table, schema, _, _, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    859\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m table \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    860\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m table, schema\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1283\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, self_destruct)\u001b[39m\n\u001b[32m   1280\u001b[39m schema: Optional[StructType] = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1281\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m-> \u001b[39m\u001b[32m1283\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1264\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1262\u001b[39m                     \u001b[38;5;28;01myield from\u001b[39;00m handle_response(b)\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1509\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   1505\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCannot invoke RPC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n\u001b[32m   1506\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectException(\n\u001b[32m   1507\u001b[39m             error_class=\u001b[33m\"\u001b[39m\u001b[33mNO_ACTIVE_SESSION\u001b[39m\u001b[33m\"\u001b[39m, message_parameters=\u001b[38;5;28mdict\u001b[39m()\n\u001b[32m   1508\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1509\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1253\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req)\u001b[39m\n\u001b[32m   1250\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1251\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_reattachable_execute:\n\u001b[32m   1252\u001b[39m         \u001b[38;5;66;03m# Don't use retryHandler - own retry handling is inside.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1253\u001b[39m         generator = \u001b[43mExecutePlanResponseReattachableIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1254\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1255\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[32m   1257\u001b[39m             \u001b[38;5;28;01myield from\u001b[39;00m handle_response(b)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/connect/client/reattach.py:123\u001b[39m, in \u001b[36mExecutePlanResponseReattachableIterator.__init__\u001b[39m\u001b[34m(self, request, stub, retry_policy, metadata)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Initial iterator comes from ExecutePlan request.\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;66;03m# Note: This is not retried, because no error would ever be thrown here, and GRPC will only\u001b[39;00m\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# throw error on first self._has_next().\u001b[39;00m\n\u001b[32m    121\u001b[39m \u001b[38;5;28mself\u001b[39m._metadata = metadata\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._iterator: Optional[Iterator[pb2.ExecutePlanResponse]] = \u001b[38;5;28miter\u001b[39m(\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stub\u001b[49m\u001b[43m.\u001b[49m\u001b[43mExecutePlan\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_initial_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    124\u001b[39m )\n\u001b[32m    126\u001b[39m \u001b[38;5;66;03m# Current item from this iterator.\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mself\u001b[39m._current: Optional[pb2.ExecutePlanResponse] = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/google/cloud/dataproc_spark_connect/client/core.py:116\u001b[39m, in \u001b[36mProxiedChannel._wrap_method.<locals>.checked_method\u001b[39m\u001b[34m(*margs, **mkwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mself\u001b[39m._is_active_callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_active_callback()\n\u001b[32m    114\u001b[39m ):\n\u001b[32m    115\u001b[39m     logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSession is no longer active\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSession not active. Please create a new session\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    118\u001b[39m     )\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped_method(*margs, **mkwargs)\n",
      "\u001b[31mRuntimeError\u001b[39m: Session not active. Please create a new session"
     ]
    }
   ],
   "source": [
    "predictions_formatted_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10ecefa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predictions_formatted_data = spark.sql(f\"\"\"SELECT\n",
    "#                        ARRAY({','.join(cols)}) as data \n",
    "#                        FROM \n",
    "#                        raw_prediction_data\n",
    "#                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5f32abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bp_args = {'project': 'wortz-project-352116', \n",
    "'location': 'us-central1',\n",
    "'endpoint_name': '2591835879202881536'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0182d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.connect.functions import pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3241b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@F.pandas_udf(ArrayType(FloatType()))\n",
    "def make_vertex_batch_predict_fn(input_data: pd.Series) -> pd.Series:\n",
    "    from google.cloud import aiplatform\n",
    "    # establish a client for each map worker\n",
    "    aiplatform.init(project=bp_args[\"project\"], location=bp_args[\"location\"])\n",
    "    logging.info(\"aiplatform client established\")\n",
    "\n",
    "    model = aiplatform.Endpoint(bp_args[\"endpoint_name\"])\n",
    "    logging.info(f\"Endpoint established: {model}\")\n",
    "    if input_data is None:\n",
    "        return None\n",
    "    float_input = input_data.apply(\n",
    "        lambda string_features: string_features.astype(float)\n",
    "    ) # cast to float\n",
    "    reshaped_input = float_input.apply(\n",
    "        lambda features: features.reshape(32, 32, 3) / 255.0\n",
    "    )  # reshape and scape per the training\n",
    "    list_typed_inputs = reshaped_input.apply(\n",
    "        lambda reshaped_arrays: reshaped_arrays.tolist()\n",
    "    ) # array to list for each element\n",
    "    response = model.predict(list_typed_inputs)\n",
    "    return pd.Series(response.predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30befa93",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SparkContext or SparkSession should be created first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m predictions_df = predictions_formatted_data.select(\u001b[43mmake_vertex_batch_predict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions_formatted_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/udf.py:423\u001b[39m, in \u001b[36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(\u001b[38;5;28mself\u001b[39m.func, assigned=assignments)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/udf.py:339\u001b[39m, in \u001b[36mUserDefinedFunction.__call__\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     sc = \u001b[43mget_active_spark_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m     profiler_enabled = sc._conf.get(\u001b[33m\"\u001b[39m\u001b[33mspark.python.profile\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     memory_profiler_enabled = sc._conf.get(\u001b[33m\"\u001b[39m\u001b[33mspark.python.profile.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/utils.py:248\u001b[39m, in \u001b[36mget_active_spark_context\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    246\u001b[39m sc = SparkContext._active_spark_context\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSparkContext or SparkSession should be created first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc\n",
      "\u001b[31mRuntimeError\u001b[39m: SparkContext or SparkSession should be created first."
     ]
    }
   ],
   "source": [
    "predictions_df = predictions_formatted_data.select(make_vertex_batch_predict_fn(predictions_formatted_data[\"data\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3e3d6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=30\n",
    "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", f\"{batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b8cbbaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "SparkContext or SparkSession should be created first.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m predictions_df = predictions_formatted_data.withColumn(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpredictions\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mmake_vertex_batch_predict_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/udf.py:423\u001b[39m, in \u001b[36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    421\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(\u001b[38;5;28mself\u001b[39m.func, assigned=assignments)\n\u001b[32m    422\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m--> \u001b[39m\u001b[32m423\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/udf.py:339\u001b[39m, in \u001b[36mUserDefinedFunction.__call__\u001b[39m\u001b[34m(self, *cols)\u001b[39m\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *cols: \u001b[33m\"\u001b[39m\u001b[33mColumnOrName\u001b[39m\u001b[33m\"\u001b[39m) -> Column:\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     sc = \u001b[43mget_active_spark_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    340\u001b[39m     profiler_enabled = sc._conf.get(\u001b[33m\"\u001b[39m\u001b[33mspark.python.profile\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     memory_profiler_enabled = sc._conf.get(\u001b[33m\"\u001b[39m\u001b[33mspark.python.profile.memory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m) == \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/model_evaluation/.venv/lib/python3.12/site-packages/pyspark/sql/utils.py:248\u001b[39m, in \u001b[36mget_active_spark_context\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    246\u001b[39m sc = SparkContext._active_spark_context\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mSparkContext or SparkSession should be created first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m sc\n",
      "\u001b[31mRuntimeError\u001b[39m: SparkContext or SparkSession should be created first."
     ]
    }
   ],
   "source": [
    "predictions_df = predictions_formatted_data.withColumn(\n",
    "        \"predictions\", make_vertex_batch_predict_fn(\"data\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9578a040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.make_vertex_batch_predict_fn(input_data: pandas.core.series.Series) -> pandas.core.series.Series>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"batch_predict\", make_vertex_batch_predict_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f19af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_formatted_data.createOrReplaceTempView(\"predictions_formatted_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad473f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = spark.sql(\"\"\"SELECT\n",
    "    batch_predict(data) as predictions\n",
    "    from predictions_formatted_data\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "54d7062e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         predictions|\n",
      "+--------------------+\n",
      "|[0.08091397, 0.10...|\n",
      "|[0.07085186, 0.09...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749079474.914156 31884138 fork_posix.cc:75] Other threads are currently calling into gRPC, skipping fork() handlers\n"
     ]
    }
   ],
   "source": [
    "predictions_df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f602913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df.write.option(\"lineSep\", \"\\n\").json(\n",
    "        f\"gs://model_experimentation_2025/output/manual_test2.jsonl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6c3edc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"predictions\":[0.08091397,0.105412275,0.12113704,0.108057655,0.114130504,0.09813027,0.08310371,0.076\"predictions\":[0.07112319,0.1044636,0.13947217,0.10541854,0.11343523,0.0878167,0.081432335,0.0735280\"predictions\":[0.076097466,0.09632193,0.13415794,0.113961145,0.108211204,0.09103544,0.079257354,0.07\"predictions\":[0.0772286,0.11498217,0.13319364,0.10327199,0.120227195,0.08881211,0.07853127,0.073229\"predictions\":[0.06878678,0.10536504,0.11724379,0.11057892,0.12136481,0.09476236,0.084734164,0.07471\"predictions\":[0.07443497,0.10469896,0.12667976,0.118034855,0.12009787,0.09318798,0.0756695,0.071637\"predictions\":[0.07898401,0.10211511,0.13486674,0.108221285,0.11681283,0.0862784,0.08065278,0.080611\"predictions\":[0.06933265,0.101526484,0.13380454,0.108027436,0.122692384,0.08855436,0.07432095,0.078"
     ]
    }
   ],
   "source": [
    "! gsutil cat -r 1-100 gs://model_experimentation_2025/output/manual_test2.jsonl/part-0000*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26cdd0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
