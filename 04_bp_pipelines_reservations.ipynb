{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12818355",
   "metadata": {},
   "source": [
    "# Batch Prediction Pipeline Workaround for Reservations\n",
    "\n",
    "Reference docs: https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.20.0/ \n",
    "\n",
    "\n",
    "This is a sample, some parameters may differ base on your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324cd9b",
   "metadata": {},
   "source": [
    "## Create custom component that deploys a model to with a GPU reservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    ")  # Common artifact types\n",
    "from google_cloud_pipeline_components.types.artifact_types import (\n",
    "    VertexEndpoint,\n",
    "    VertexModel,\n",
    ")\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\"],\n",
    ")\n",
    "def create_endpoint_with_reservation(\n",
    "    endpoint: Input[VertexEndpoint],\n",
    "    model: str,\n",
    "    deployed_name: str,\n",
    "    machine_type: str,\n",
    "    accelerator_type: str,\n",
    "    accelerator_count: int,\n",
    "    reservation_zone: str,\n",
    "    project_id: str,\n",
    "    reservation_name: str,\n",
    "    min_replica: int,\n",
    "    max_replica: int,\n",
    "    location: str,\n",
    "    deployed_model: Output[VertexModel],\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Deploys a model to an existing Vertex AI endpoint with a specific reservation.\n",
    "\n",
    "    Args:\n",
    "        endpoint: The Vertex AI endpoint to deploy to.\n",
    "        model: The model ID to deploy.\n",
    "        deployed_name: The display name for the deployed model.\n",
    "        machine_type: The machine type for the deployed model.\n",
    "        accelerator_type: The accelerator type for the deployed model.\n",
    "        accelerator_count: The number of accelerators for the deployed model.\n",
    "        reservation_zone: The zone of the reservation.\n",
    "        project_id: The project ID.\n",
    "        reservation_name: The name of the reservation.\n",
    "        min_replica: The minimum number of replicas.\n",
    "        max_replica: The maximum number of replicas.\n",
    "        location: The location of the endpoint and model.\n",
    "        deployed_endpoint: The output artifact representing the deployed endpoint.\n",
    "        deployed_model: The output artifact representing the deployed model.\n",
    "\n",
    "    Returns:\n",
    "        deployed_endpoint: Output[VertexEndpoint],\n",
    "        deployed_model: Output[VertexModel],\n",
    "    \"\"\"\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    endpoint_fqn = endpoint.uri.split(\"v1/\")[1]\n",
    "    model_fqn = f\"projects/{project_id}/locations/{location}/models/{model}\"\n",
    "    vertex_endpoint = aiplatform.Endpoint(endpoint_fqn)\n",
    "    vertex_model = aiplatform.Model(model_name=model_fqn)\n",
    "\n",
    "    vertex_endpoint.deploy(\n",
    "        model=vertex_model,\n",
    "        deployed_model_display_name=deployed_name,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        reservation_affinity_type=\"SPECIFIC_RESERVATION\",\n",
    "        reservation_affinity_key=\"compute.googleapis.com/reservation-name\",\n",
    "        reservation_affinity_values=[\n",
    "            f\"projects/{project_id}/zones/{reservation_zone}/reservations/{reservation_name}\"\n",
    "        ],\n",
    "        min_replica_count=min_replica,\n",
    "        max_replica_count=max_replica,\n",
    "        sync=True,\n",
    "    )\n",
    "    # return types\n",
    "    deployed_model.uri = f\"https://{location}-aiplatform.googleapis.com/v1/{model_fqn}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614eb0f4",
   "metadata": {},
   "source": [
    "## Build another custom component that does the batch prediction from a gcs location\n",
    "Note this is has some specific data manipulation to this model and may be different for other implementations\n",
    "\n",
    "```bash\n",
    "curl -L -o ~/Downloads/cifar10-python-in-csv.zip\\\n",
    "  https://www.kaggle.com/api/v1/datasets/download/fedesoriano/cifar10-python-in-csv\n",
    "```\n",
    "\n",
    "Unzip the file then upload the test file to the bucket.\n",
    "\n",
    "```bash\n",
    "gsutil cp test.csv gs://model_experimentation_2025/prediction_data/test.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac26830f",
   "metadata": {},
   "source": [
    "#### Create a custom Dataproc serverless component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    base_image=\"python:3.11\",\n",
    "    packages_to_install=[\n",
    "        \"dataproc-spark-connect\",\n",
    "        \"google-cloud-aiplatform\",\n",
    "        \"google-cloud-pipeline-components\",\n",
    "    ],\n",
    ")\n",
    "def custom_batch_predict(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    endpoint: Input[VertexEndpoint],\n",
    "    bucket: str,\n",
    "    prediction_blob: str,\n",
    "    destination_blob: str,\n",
    "    batch_size: int,\n",
    "    dataproc_serverless_template: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    Custom batch prediction component using Dataproc Serverless.\n",
    "\n",
    "    Args:\n",
    "        project_id: Project ID of the Google Cloud project.\n",
    "        location: Location of the Google Cloud project.\n",
    "        endpoint: Vertex AI Endpoint resource.\n",
    "        bucket: GCS bucket to read the input data from and write the predictions to.\n",
    "        prediction_blob: GCS blob path for the input data.\n",
    "        destination_blob: GCS blob path to write the predictions to.\n",
    "        batch_size: Number of rows to process per batch.\n",
    "        dataproc_serverless_template: Dataproc Serverless session template name.\n",
    "\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from pyspark.sql.connect.types import FloatType, ArrayType\n",
    "    import logging\n",
    "    import pyspark.sql.connect.functions as F\n",
    "    from google.cloud.dataproc_v1 import Session\n",
    "\n",
    "    session_config = Session()\n",
    "    session_config.session_template = dataproc_serverless_template\n",
    "\n",
    "    from google.cloud.dataproc_spark_connect import DataprocSparkSession\n",
    "\n",
    "    spark = (\n",
    "        DataprocSparkSession.builder.projectId(project_id)\n",
    "        .location(location)\n",
    "        .dataprocSessionConfig(session_config)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    # set the batch size for number of rows handled per `predict` request\n",
    "    spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", f\"{batch_size}\")\n",
    "    # sc = pyspark.SparkContext.getOrCreate()\n",
    "\n",
    "    df = spark.read.option(\"header\", True).csv(f\"gs://{bucket}/{prediction_blob}\")\n",
    "\n",
    "    prediction_data = df.drop(\"label\")\n",
    "\n",
    "    cols = prediction_data.columns\n",
    "\n",
    "    predictions_formatted_data = prediction_data.withColumn(\n",
    "        \"data\", F.array(*cols)\n",
    "    ).drop(*cols)\n",
    "\n",
    "    endpoint_id = endpoint.uri.split(\"/\")[-1]\n",
    "\n",
    "    bp_args = {\n",
    "        \"project\": project_id,\n",
    "        \"location\": location,\n",
    "        \"endpoint_name\": endpoint_id,\n",
    "    }\n",
    "\n",
    "    # this will vary by models - follow guidance here for prediction formats: https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-online-predictions\n",
    "    @F.pandas_udf(ArrayType(FloatType()))\n",
    "    def make_vertex_batch_predict_fn(input_data: pd.Series) -> pd.Series:\n",
    "        from google.cloud import aiplatform\n",
    "\n",
    "        # establish a client for each map worker\n",
    "        aiplatform.init(project=bp_args[\"project\"], location=bp_args[\"location\"])\n",
    "        logging.info(\"aiplatform client established\")\n",
    "\n",
    "        model = aiplatform.Endpoint(bp_args[\"endpoint_name\"])\n",
    "        logging.info(f\"Endpoint established: {model}\")\n",
    "        if input_data is None:\n",
    "            return None\n",
    "        float_input = input_data.apply(\n",
    "            lambda string_features: string_features.astype(float)\n",
    "        )  # cast to float\n",
    "        reshaped_input = float_input.apply(\n",
    "            lambda features: features.reshape(32, 32, 3) / 255.0\n",
    "        )  # reshape and scape per the training\n",
    "        list_typed_inputs = reshaped_input.apply(\n",
    "            lambda reshaped_arrays: reshaped_arrays.tolist()\n",
    "        )  # array to list for each element\n",
    "        response = model.predict(list_typed_inputs.values.tolist())\n",
    "        return pd.Series(response.predictions)\n",
    "\n",
    "    spark.udf.register(\"batch_predict\", make_vertex_batch_predict_fn)\n",
    "\n",
    "    predictions_formatted_data.createOrReplaceTempView(\"predictions_formatted_data\")\n",
    "\n",
    "    # TODO: Error handling for quota/429s with backoff strategies\n",
    "\n",
    "    predictions_df = spark.sql(\n",
    "        \"\"\"SELECT\n",
    "    batch_predict(data) as predictions, data\n",
    "    from predictions_formatted_data\n",
    "    \"\"\"\n",
    "    )\n",
    "    top2_qa_result = predictions_df.show(2)\n",
    "\n",
    "    logging.info(f\"top two results: \\n{top2_qa_result}\")\n",
    "\n",
    "    # write the predictions to gcs\n",
    "    predictions_df.write.option(\"lineSep\", \"\\n\").json(\n",
    "        f\"gs://{bucket}/{destination_blob}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1dd3e",
   "metadata": {},
   "source": [
    "## Important - make sure you enable `roles/compute.viewer` permissions for the Vertex Service Account\n",
    "\n",
    "This is important so the tenant project running the pipeline can view and utilize the reservation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57daab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated IAM policy for reservation [a100-custom-image-reservation].\n",
      "bindings:\n",
      "- members:\n",
      "  - serviceAccount:vertex-sa@wortz-project-352116.iam.gserviceaccount.com\n",
      "  role: roles/compute.admin\n",
      "- members:\n",
      "  - serviceAccount:vertex-sa@wortz-project-352116.iam.gserviceaccount.com\n",
      "  role: roles/compute.futureReservationAdmin\n",
      "- members:\n",
      "  - serviceAccount:vertex-sa@wortz-project-352116.iam.gserviceaccount.com\n",
      "  role: roles/compute.instanceAdmin\n",
      "- members:\n",
      "  - serviceAccount:679926387543-compute@developer.gserviceaccount.com\n",
      "  - serviceAccount:service-679926387543@gcp-sa-aiplatform.iam.gserviceaccount.com\n",
      "  - serviceAccount:vertex-sa@wortz-project-352116.iam.gserviceaccount.com\n",
      "  role: roles/compute.viewer\n",
      "etag: BwY2yUa0B5c=\n",
      "version: 1\n"
     ]
    }
   ],
   "source": [
    "#### Bind SA to the reservation\n",
    "PROJECT_NUMBER = 679926387543\n",
    "\n",
    "! gcloud compute reservations add-iam-policy-binding \\\n",
    "    a100-custom-image-reservation \\\n",
    "    --zone=us-central1-b \\\n",
    "    --member=\"serviceAccount:service-$PROJECT_NUMBER@gcp-sa-aiplatform.iam.gserviceaccount.com\" \\\n",
    "    --role=\"roles/compute.viewer\" \\\n",
    "    --project=wortz-project-352116"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c45b22",
   "metadata": {},
   "source": [
    "## Pipeline with standard components integrated into the custom reservation deploy and batch predict\n",
    "\n",
    "**Important**: create a Dataproc Template that uses default compute service account: `$PROJECT_NUMBER-compute@developer.gserviceaccount.com`\n",
    "\n",
    "<img src='img/serverless_template.png' width=300px/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from google_cloud_pipeline_components.v1.endpoint import (\n",
    "    EndpointCreateOp,\n",
    "    EndpointDeleteOp,\n",
    "    ModelUndeployOp,\n",
    ")\n",
    "\n",
    "bucket = (\"model_experimentation_2025\",)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"deploy-model-with-reserved-gpu\",\n",
    "    description=\"Deploys a model to an endpoint using a reserved GPU.\",\n",
    ")\n",
    "def deploy_model_pipeline(\n",
    "    project_id: str,\n",
    "    model: str,\n",
    "    region: str,\n",
    "    zone: str,\n",
    "    reservation_name: str,\n",
    "    endpoint_display_name: str,\n",
    "    deployed_model_display_name: str,\n",
    "    machine_type: str,\n",
    "    accelerator_type: str,\n",
    "    bucket: str,\n",
    "    prediction_input_blob: str,\n",
    "    prediction_output_blob: str,\n",
    "    dataproc_serverless_template: str,\n",
    "    batch_size: int,\n",
    "    accelerator_count: int,\n",
    "    min_replica: int,\n",
    "    max_replica: int,\n",
    "):\n",
    "\n",
    "    # 1. Create an endpoint\n",
    "    create_endpoint_op = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        display_name=endpoint_display_name,\n",
    "    ).set_display_name(\"Create an Endpoint\")\n",
    "\n",
    "    # 2. Deploy the model to the endpoint with reserved GPU\n",
    "    model_deploy_op = create_endpoint_with_reservation(\n",
    "        endpoint=create_endpoint_op.outputs[\"endpoint\"],\n",
    "        model=model,\n",
    "        location=region,\n",
    "        deployed_name=deployed_model_display_name,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        reservation_zone=zone,\n",
    "        project_id=project_id,\n",
    "        reservation_name=reservation_name,\n",
    "        min_replica=min_replica,\n",
    "        max_replica=max_replica,\n",
    "    ).set_display_name(\"Deploy with GPU Reservation\")\n",
    "\n",
    "    # 3. Dataproc spark-based batch prediction job here\n",
    "    batch_predict_op = (\n",
    "        custom_batch_predict(\n",
    "            project_id=project_id,\n",
    "            location=region,\n",
    "            endpoint=create_endpoint_op.outputs[\"endpoint\"],\n",
    "            bucket=bucket,\n",
    "            prediction_blob=prediction_input_blob,\n",
    "            destination_blob=prediction_output_blob,\n",
    "            batch_size=batch_size,\n",
    "            dataproc_serverless_template=dataproc_serverless_template,\n",
    "        )\n",
    "        .after(model_deploy_op)\n",
    "        .set_display_name(\"Batch Predict With Dataproc Serverless\")\n",
    "    )\n",
    "\n",
    "    # 4. Teardown of resources post-prediction\n",
    "    model_undeploy_op = (\n",
    "        ModelUndeployOp(\n",
    "            endpoint=create_endpoint_op.outputs[\"endpoint\"],\n",
    "            model=model_deploy_op.outputs[\"deployed_model\"],\n",
    "            # traffic_split={\"0\": 100} # Optional: to ensure all traffic is removed from this model_id\n",
    "            # If this is the only model, it will be removed.\n",
    "        )\n",
    "        .set_display_name(\"Undeploy Model\")\n",
    "        .after(batch_predict_op)\n",
    "    )\n",
    "\n",
    "    EndpointDeleteOp(\n",
    "        endpoint=create_endpoint_op.outputs[\n",
    "            \"endpoint\"\n",
    "        ],  # Use the same endpoint from deploy op\n",
    "    ).set_display_name(\"Undeploy Endpoint\").after(\n",
    "        model_undeploy_op\n",
    "    )  # Explicitly set dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482b06f",
   "metadata": {},
   "source": [
    "## Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2f332a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=deploy_model_pipeline,\n",
    "    package_path=\"predict_w_reservations.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacc3429",
   "metadata": {},
   "source": [
    "#### Set the pipeline parameters dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9df8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your project ID, region, etc.\n",
    "import time\n",
    "\n",
    "epoch_time = time.time()\n",
    "pipeline_params = dict(\n",
    "    project_id=\"wortz-project-352116\",\n",
    "    model=\"3416616934593003520\",\n",
    "    region=\"us-central1\",\n",
    "    zone=\"us-central1-b\",\n",
    "    reservation_name=\"a100-custom-image-reservation\",\n",
    "    endpoint_display_name=\"Reservation_Endpoint\",\n",
    "    deployed_model_display_name=\"My_deployed_model\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_A100\",  # find these settings here https://cloud.google.com/compute/docs/accelerator-optimized-machines\n",
    "    machine_type=\"a2-highgpu-1g\",\n",
    "    bucket=\"model_experimentation_2025\",\n",
    "    prediction_input_blob=\"prediction_data/test.csv\",\n",
    "    # prediction_output_blob=f\"output_data/predictions_{epoch_time}.jsonl\",\n",
    "    prediction_output_blob=f\"output_data/predictions_1749091255.759079.jsonl\",\n",
    "    dataproc_serverless_template=\"projects/wortz-project-352116/locations/us-central1/sessionTemplates/spark-connect-2-2\",\n",
    "    batch_size=30,  # set batch size to fit in .predict's 1.5 Mb limit\n",
    "    accelerator_count=1,\n",
    "    min_replica=1,\n",
    "    max_replica=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dfcd1a",
   "metadata": {},
   "source": [
    "# Run the pipeline with the compute SA \n",
    "The compute SA is chosen because it was used for the Dataproc Serverless Template, other SAs when configuring new templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3e6654d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/679926387543/locations/us-central1/pipelineJobs/deploy-model-with-reserved-gpu-20250604214058\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/679926387543/locations/us-central1/pipelineJobs/deploy-model-with-reserved-gpu-20250604214058')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/deploy-model-with-reserved-gpu-20250604214058?project=679926387543\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=pipeline_params[\"project_id\"],\n",
    "    location=pipeline_params[\"region\"],\n",
    ")\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"Predictions with GPU Reservations\",\n",
    "    template_path=\"predict_w_reservations.json\",\n",
    "    parameter_values=pipeline_params,\n",
    "    project=pipeline_params[\"project_id\"],\n",
    "    location=pipeline_params[\"region\"],\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "job.submit(service_account=\"679926387543-compute@developer.gserviceaccount.com\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b481ba1",
   "metadata": {},
   "source": [
    "## Validate the output predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "517741ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"data\":[\"158\",\"159\",\"165\",\"166\",\"160\",\"156\",\"162\",\"159\",\"158\",\"159\",\"161\",\"160\",\"161\",\"166\",\"169\",\"1\"data\":[\"203\",\"206\",\"210\",\"211\",\"208\",\"204\",\"202\",\"199\",\"199\",\"206\",\"208\",\"208\",\"208\",\"205\",\"207\",\"2\"data\":[\"158\",\"165\",\"168\",\"174\",\"170\",\"173\",\"187\",\"201\",\"204\",\"209\",\"213\",\"217\",\"220\",\"222\",\"222\",\"2\"data\":[\"160\",\"154\",\"145\",\"160\",\"103\",\"82\",\"115\",\"133\",\"151\",\"141\",\"140\",\"93\",\"93\",\"103\",\"146\",\"99\",\"data\":[\"156\",\"238\",\"255\",\"255\",\"255\",\"255\",\"249\",\"249\",\"181\",\"140\",\"141\",\"137\",\"108\",\"139\",\"147\",\"1\"data\":[\"75\",\"89\",\"102\",\"75\",\"63\",\"107\",\"161\",\"174\",\"127\",\"160\",\"174\",\"172\",\"220\",\"197\",\"181\",\"213\",\"data\":[\"131\",\"135\",\"136\",\"139\",\"140\",\"141\",\"143\",\"146\",\"147\",\"142\",\"117\",\"105\",\"130\",\"170\",\"159\",\"1\"data\":[\"226\",\"223\",\"223\",\"225\",\"226\",\"227\",\"228\",\"227\",\"227\",\"227\",\"227\",\"227\",\"227\",\"227\",\"227\",\"2"
     ]
    }
   ],
   "source": [
    "bucket = pipeline_params[\"bucket\"]\n",
    "blob = pipeline_params[\"prediction_output_blob\"]\n",
    "! gsutil cat -r 1-100 gs://$bucket/$blob/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6e4f4a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://{project_id: wortz-project-352116, model: 3416616934593003520, region: us-central1, zone: us-central1-b, reservation_name: a100-custom-image-reservation, endpoint_display_name: Reservation_Endpoint, deployed_model_display_name: My_deployed_model, accelerator_type: NVIDIA_TESLA_A100, machine_type: a2-highgpu-1g, bucket: model_experimentation_2025, prediction_input_blob: prediction_data/test.csv, prediction_output_blob: output_data/predictions_1749087963.064943.jsonl, dataproc_serverless_template: projects/wortz-project-352116/locations/us-central1/sessionTemplates/spark-connect-2-2, batch_size: 30, accelerator_count: 1, min_replica: 1, max_replica: 2}[bucket]{project_id: wortz-project-352116, model: 3416616934593003520, region: us-central1, zone: us-central1-b, reservation_name: a100-custom-image-reservation, endpoint_display_name: Reservation_Endpoint, deployed_model_display_name: My_deployed_model, accelerator_type: NVIDIA_TESLA_A100, machine_type: a2-highgpu-1g, bucket: model_experimentation_2025, prediction_input_blob: prediction_data/test.csv, prediction_output_blob: output_data/predictions_1749087963.064943.jsonl, dataproc_serverless_template: projects/wortz-project-352116/locations/us-central1/sessionTemplates/spark-connect-2-2, batch_size: 30, accelerator_count: 1, min_replica: 1, max_replica: 2}[prediction_output_blob]/part-0000*\n"
     ]
    }
   ],
   "source": [
    "! echo gs://$pipeline_params[\"bucket\"]$pipeline_params[\"prediction_output_blob\"]/part-0000*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f5411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
