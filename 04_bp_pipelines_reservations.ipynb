{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12818355",
   "metadata": {},
   "source": [
    "# Batch Prediction Pipeline Workaround for Reservations\n",
    "\n",
    "Reference docs: https://google-cloud-pipeline-components.readthedocs.io/en/google-cloud-pipeline-components-2.20.0/ \n",
    "\n",
    "\n",
    "This is a sample, some parameters may differ base on your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3324cd9b",
   "metadata": {},
   "source": [
    "## Create custom component that deploys a model to with a GPU reservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    ")  # Common artifact types\n",
    "from google_cloud_pipeline_components.types.artifact_types import (\n",
    "    VertexEndpoint,\n",
    "    VertexModel,\n",
    ")\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "    packages_to_install=[\"google-cloud-aiplatform\", \"google-cloud-pipeline-components\"]\n",
    ")\n",
    "def create_endpoint_with_reservation(\n",
    "    endpoint: Input[VertexEndpoint],\n",
    "    model: str,\n",
    "    deployed_name: str,\n",
    "    machine_type: str,\n",
    "    accelerator_type: str,\n",
    "    accelerator_count: int,\n",
    "    reservation_zone: str,\n",
    "    project_id: str,\n",
    "    reservation_name: str,\n",
    "    min_replica: int,\n",
    "    max_replica: int,\n",
    "    location: str,\n",
    "    deployed_endpoint: Output[VertexEndpoint],\n",
    "    deployed_model: Output[VertexModel],\n",
    ") -> dict:\n",
    "    from google_cloud_pipeline_components.types.artifact_types import (\n",
    "        VertexModel,\n",
    "    )\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    endpoint_fqn = endpoint.uri.split(\"v1/\")[1]\n",
    "    model_fqn = f\"projects/{project_id}/locations/{location}/models/{model}\"\n",
    "    vertex_endpoint = aiplatform.Endpoint(endpoint_fqn)\n",
    "    vertex_model = aiplatform.Model(model_name=model_fqn)\n",
    "\n",
    "    vertex_endpoint.deploy(\n",
    "        model=vertex_model,\n",
    "        deployed_model_display_name=deployed_name,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        reservation_affinity_type=\"SPECIFIC_RESERVATION\",\n",
    "        reservation_affinity_key=\"compute.googleapis.com/reservation-name\",\n",
    "        reservation_affinity_values=[\n",
    "            f\"projects/{project_id}/zones/{reservation_zone}/reservations/{reservation_name}\"\n",
    "        ],\n",
    "        min_replica_count=min_replica,\n",
    "        max_replica_count=max_replica,\n",
    "        sync=True,\n",
    "    )\n",
    "    return {\n",
    "        \"deployed_endpoint\": endpoint,\n",
    "        \"deployed_model\": VertexModel(model_resource_name=model),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca9209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "from google_cloud_pipeline_components.v1.endpoint import (\n",
    "    EndpointCreateOp,\n",
    "    EndpointDeleteOp,\n",
    "    ModelUndeployOp,\n",
    ")\n",
    "\n",
    "bucket = (\"model_experimentation_2025\",)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"deploy-model-with-reserved-gpu\",\n",
    "    description=\"Deploys a model to an endpoint using a reserved GPU.\",\n",
    "    # pipeline_root=f\"gs://{bucket}/pipeline_root\",\n",
    ")\n",
    "def deploy_model_pipeline(\n",
    "    project_id: str,\n",
    "    model: str,\n",
    "    region: str,\n",
    "    shared_project_id: str,\n",
    "    zone: str,\n",
    "    reservation_name: str,\n",
    "    endpoint_display_name: str,\n",
    "    deployed_model_display_name: str,\n",
    "    machine_type: str,\n",
    "    accelerator_type: str,\n",
    "):\n",
    "\n",
    "    # 1. Create an endpoint\n",
    "    create_endpoint_task = EndpointCreateOp(\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        display_name=endpoint_display_name,\n",
    "    )\n",
    "\n",
    "    # 2. Deploy the model to the endpoint with reserved GPU\n",
    "    model_deploy_op = create_endpoint_with_reservation(\n",
    "        endpoint=create_endpoint_task.outputs[\"endpoint\"],\n",
    "        model=model,\n",
    "        location=region,\n",
    "        deployed_name=deployed_model_display_name,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=1,\n",
    "        reservation_zone=zone,\n",
    "        project_id=shared_project_id,\n",
    "        reservation_name=reservation_name,\n",
    "        min_replica=1,\n",
    "        max_replica=2,\n",
    "    )\n",
    "    # 3. TODO: Implement prediction job here via custom component\n",
    "\n",
    "    # 4. Teardown of resources post-prediction\n",
    "    model_undeploy_op = ModelUndeployOp(\n",
    "        endpoint=model_deploy_op.outputs[\"deployed_endpoint\"],\n",
    "        model=model_deploy_op.outputs[\"deployed_model\"],\n",
    "        # traffic_split={\"0\": 100} # Optional: to ensure all traffic is removed from this model_id\n",
    "        # If this is the only model, it will be removed.\n",
    "    )\n",
    "\n",
    "    delete_endpoint_op = EndpointDeleteOp(\n",
    "        endpoint=model_deploy_op.outputs[\n",
    "            \"deployed_endpoint\"\n",
    "        ],  # Use the same endpoint from deploy op\n",
    "    )\n",
    "    delete_endpoint_op.after(model_undeploy_op)  # Explicitly set dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5482b06f",
   "metadata": {},
   "source": [
    "## Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f332a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler().compile(\n",
    "    pipeline_func=deploy_model_pipeline,\n",
    "    package_path=\"predict_w_reservations.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9df8a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with your project ID, region, etc.\n",
    "pipeline_params = dict(\n",
    "    project_id=\"wortz-project-352116\",\n",
    "    model=\"3416616934593003520\",\n",
    "    region=\"us-central1\",\n",
    "    shared_project_id=\"wortz-project-352116\",\n",
    "    zone=\"us-central1-b\",\n",
    "    reservation_name=\"a100-custom-image-reservation\",\n",
    "    endpoint_display_name=\"Reservation_Endpoint\",\n",
    "    deployed_model_display_name=\"My_deployed_model\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_A100\",\n",
    "    machine_type=\"a2-highgpu-1g\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6654d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/679926387543/locations/us-central1/pipelineJobs/deploy-model-with-reserved-gpu-20250522183049\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/679926387543/locations/us-central1/pipelineJobs/deploy-model-with-reserved-gpu-20250522183049')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/deploy-model-with-reserved-gpu-20250522183049?project=679926387543\n"
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=pipeline_params[\"project_id\"],\n",
    "    location=pipeline_params[\"region\"],\n",
    ")\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=f\"Predictions with GPU Reservations\",\n",
    "    template_path=\"predict_w_reservations.json\",\n",
    "    # pipeline_root=f\"gs://{bucket}/pipeline_runs\",\n",
    "    parameter_values=pipeline_params,\n",
    "    project=pipeline_params[\"project_id\"],\n",
    "    location=pipeline_params[\"region\"],\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "job.submit(service_account=\"vertex-sa@wortz-project-352116.iam.gserviceaccount.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9c005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "one_sample = datasets[\"test\"].map(scale).take(1)\n",
    "list_data = list(one_sample.as_numpy_iterator())\n",
    "instances = {\"instances\": list_data[0][0].tolist()}\n",
    "json_str = json.dumps(instances)\n",
    "with open(\"data.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(json_str, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743017f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
