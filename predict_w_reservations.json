{
  "components": {
    "comp-create-endpoint-with-reservation": {
      "executorLabel": "exec-create-endpoint-with-reservation",
      "inputDefinitions": {
        "artifacts": {
          "endpoint": {
            "artifactType": {
              "schemaTitle": "google.VertexEndpoint",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "accelerator_count": {
            "parameterType": "NUMBER_INTEGER"
          },
          "accelerator_type": {
            "parameterType": "STRING"
          },
          "deployed_name": {
            "parameterType": "STRING"
          },
          "location": {
            "parameterType": "STRING"
          },
          "machine_type": {
            "parameterType": "STRING"
          },
          "max_replica": {
            "parameterType": "NUMBER_INTEGER"
          },
          "min_replica": {
            "parameterType": "NUMBER_INTEGER"
          },
          "model": {
            "parameterType": "STRING"
          },
          "project_id": {
            "parameterType": "STRING"
          },
          "reservation_name": {
            "parameterType": "STRING"
          },
          "reservation_zone": {
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "deployed_endpoint": {
            "artifactType": {
              "schemaTitle": "google.VertexEndpoint",
              "schemaVersion": "0.0.1"
            }
          },
          "deployed_model": {
            "artifactType": {
              "schemaTitle": "google.VertexModel",
              "schemaVersion": "0.0.1"
            }
          }
        },
        "parameters": {
          "endpoint_id": {
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-dataproc-create-pyspark-batch": {
      "executorLabel": "exec-dataproc-create-pyspark-batch",
      "inputDefinitions": {
        "parameters": {
          "archive_uris": {
            "defaultValue": [],
            "description": "HCFS URIs of archives to be extracted into the working directory of each executor. Supported file types: `.jar`, `.tar`, `.tar.gz`, `.tgz`, and `.zip`.",
            "isOptional": true,
            "parameterType": "LIST"
          },
          "args": {
            "defaultValue": [],
            "description": "The arguments to pass to the driver. Do not include arguments that can be set as batch properties, such as `--conf`, since a collision can occur that causes an incorrect batch submission.",
            "isOptional": true,
            "parameterType": "LIST"
          },
          "batch_id": {
            "defaultValue": "",
            "description": "The ID to use for the batch, which will become the final component of the batch's resource name. If none is specified, a default name will be generated by the component.  This value must be 4-63 characters. Valid characters are `/[a-z][0-9]-/`.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "container_image": {
            "defaultValue": "",
            "description": "Optional custom container image for the job runtime environment. If not specified, a default container image will be used.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "file_uris": {
            "defaultValue": [],
            "description": "HCFS URIs of files to be placed in the working directory of each executor.",
            "isOptional": true,
            "parameterType": "LIST"
          },
          "jar_file_uris": {
            "defaultValue": [],
            "description": "HCFS URIs of jar files to add to the classpath of the Spark driver and tasks.",
            "isOptional": true,
            "parameterType": "LIST"
          },
          "kms_key": {
            "defaultValue": "",
            "description": "The Cloud KMS key to use for encryption.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "labels": {
            "defaultValue": {},
            "description": "The labels to associate with this batch. Label keys must contain 1 to 63 characters, and must conform to RFC 1035. Label values may be empty, but, if present, must contain 1 to 63 characters, and must conform to RFC 1035. No more than 32 labels can be associated with a batch.  An object containing a list of `\"key\": value` pairs. Example: `{ \"name\": \"wrench\", \"mass\": \"1.3kg\", \"count\": \"3\" }`.",
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "location": {
            "defaultValue": "us-central1",
            "description": "Location of the Dataproc batch workload. If not set, defaults to `\"us-central1\"`.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "main_python_file_uri": {
            "description": "The HCFS URI of the main Python file to use as the Spark driver. Must be a `.py` file.",
            "parameterType": "STRING"
          },
          "metastore_service": {
            "defaultValue": "",
            "description": "Resource name of an existing Dataproc Metastore service.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "network_tags": {
            "defaultValue": [],
            "description": "Tags used for network traffic control.",
            "isOptional": true,
            "parameterType": "LIST"
          },
          "network_uri": {
            "defaultValue": "",
            "description": "Network URI to connect workload to.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project": {
            "defaultValue": "{{$.pipeline_google_cloud_project_id}}",
            "description": "Project to run the Dataproc batch workload. Defaults to the project in which the PipelineJob is run.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "python_file_uris": {
            "defaultValue": [],
            "description": "HCFS file URIs of Python files to pass to the PySpark framework. Supported file types: `.py`, `.egg`, and `.zip`.",
            "isOptional": true,
            "parameterType": "LIST"
          },
          "runtime_config_properties": {
            "defaultValue": {},
            "description": "Runtime configuration for the workload.",
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "runtime_config_version": {
            "defaultValue": "",
            "description": "Version of the batch runtime.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "service_account": {
            "defaultValue": "",
            "description": "Service account that is used to execute the workload.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "spark_history_dataproc_cluster": {
            "defaultValue": "",
            "description": "The Spark History Server configuration for the workload.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "subnetwork_uri": {
            "defaultValue": "",
            "description": "Subnetwork URI to connect workload to.",
            "isOptional": true,
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "gcp_resources": {
            "description": "Serialized gcp_resources proto tracking the Dataproc batch workload. For more details, see https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/google_cloud_pipeline_components/proto/README.md.",
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-endpoint-create": {
      "executorLabel": "exec-endpoint-create",
      "inputDefinitions": {
        "parameters": {
          "description": {
            "defaultValue": "",
            "description": "The description of the Endpoint.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "display_name": {
            "description": "The user-defined name of the Endpoint. The name can be up to 128 characters long and can be consist of any UTF-8 characters.",
            "parameterType": "STRING"
          },
          "encryption_spec_key_name": {
            "defaultValue": "",
            "description": "Customer-managed encryption key spec for an Endpoint. If set, this Endpoint and all of this Endoint's sub-resources will be secured by this key. Has the form: `projects/my-project/locations/my-location/keyRings/my-kr/cryptoKeys/my-key`. The key needs to be in the same region as where the compute resource is created.  If set, this Endpoint and all sub-resources of this Endpoint will be secured by this key.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "labels": {
            "defaultValue": {},
            "description": "The labels with user-defined metadata to organize your Endpoints.  Label keys and values can be no longer than 64 characters (Unicode codepoints), can only contain lowercase letters, numeric characters, underscores and dashes. International characters are allowed.  See https://goo.gl/xmQnxf for more information and examples of labels.",
            "isOptional": true,
            "parameterType": "STRUCT"
          },
          "location": {
            "defaultValue": "us-central1",
            "description": "Location to create the Endpoint. If not set, default to us-central1.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "network": {
            "defaultValue": "",
            "description": "The full name of the Google Compute Engine network to which the Endpoint should be peered. Private services access must already be configured for the network. If left unspecified, the Endpoint is not peered with any network. [Format](https://cloud.google.com/compute/docs/reference/rest/v1/networks/insert): `projects/{project}/global/networks/{network}`. Where `{project}` is a project number, as in `'12345'`, and `{network}` is network name.",
            "isOptional": true,
            "parameterType": "STRING"
          },
          "project": {
            "defaultValue": "{{$.pipeline_google_cloud_project_id}}",
            "description": "Project to create the Endpoint. Defaults to the project in which the PipelineJob is run.",
            "isOptional": true,
            "parameterType": "STRING"
          }
        }
      },
      "outputDefinitions": {
        "artifacts": {
          "endpoint": {
            "artifactType": {
              "schemaTitle": "google.VertexEndpoint",
              "schemaVersion": "0.0.1"
            },
            "description": "Artifact tracking the created Endpoint."
          }
        },
        "parameters": {
          "gcp_resources": {
            "description": "Serialized JSON of `gcp_resources` [proto](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/proto) which tracks the create Endpoint's long-running operation.",
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-endpoint-delete": {
      "executorLabel": "exec-endpoint-delete",
      "inputDefinitions": {
        "artifacts": {
          "endpoint": {
            "artifactType": {
              "schemaTitle": "google.VertexEndpoint",
              "schemaVersion": "0.0.1"
            },
            "description": "The Endpoint to be deleted."
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "gcp_resources": {
            "description": "Serialized JSON of `gcp_resources` [proto](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/proto) which tracks the delete Endpoint's long-running operation.",
            "parameterType": "STRING"
          }
        }
      }
    },
    "comp-model-undeploy": {
      "executorLabel": "exec-model-undeploy",
      "inputDefinitions": {
        "artifacts": {
          "endpoint": {
            "artifactType": {
              "schemaTitle": "google.VertexEndpoint",
              "schemaVersion": "0.0.1"
            },
            "description": "The Endpoint for the DeployedModel to be undeployed from."
          },
          "model": {
            "artifactType": {
              "schemaTitle": "google.VertexModel",
              "schemaVersion": "0.0.1"
            },
            "description": "The model that was deployed to the Endpoint."
          }
        },
        "parameters": {
          "traffic_split": {
            "defaultValue": {},
            "description": "If this field is provided, then the Endpoint's trafficSplit will be overwritten with it. If last DeployedModel is being undeployed from the Endpoint, the [Endpoint.traffic_split] will always end up empty when this call returns. A DeployedModel will be successfully undeployed only if it doesn't have any traffic assigned to it when this method executes, or if this field unassigns any traffic to it.",
            "isOptional": true,
            "parameterType": "STRUCT"
          }
        }
      },
      "outputDefinitions": {
        "parameters": {
          "gcp_resources": {
            "description": "Serialized JSON of `gcp_resources` [proto](https://github.com/kubeflow/pipelines/tree/master/components/google-cloud/google_cloud_pipeline_components/proto) which tracks the undeploy Model's long-running operation.",
            "parameterType": "STRING"
          }
        }
      }
    }
  },
  "deploymentSpec": {
    "executors": {
      "exec-create-endpoint-with-reservation": {
        "container": {
          "args": [
            "--executor_input",
            "{{$}}",
            "--function_to_execute",
            "create_endpoint_with_reservation"
          ],
          "command": [
            "sh",
            "-c",
            "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'kfp==2.13.0' '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&  python3 -m pip install --quiet --no-warn-script-location 'google-cloud-aiplatform' 'google-cloud-pipeline-components' && \"$0\" \"$@\"\n",
            "sh",
            "-ec",
            "program_path=$(mktemp -d)\n\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\n_KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\n",
            "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import *\nfrom google_cloud_pipeline_components.types.artifact_types import VertexEndpoint\nfrom google_cloud_pipeline_components.types.artifact_types import VertexModel\nfrom builtins import str\n\ndef create_endpoint_with_reservation(\n    endpoint: Input[VertexEndpoint],\n    model: str,\n    deployed_name: str,\n    machine_type: str,\n    accelerator_type: str,\n    accelerator_count: int,\n    reservation_zone: str,\n    project_id: str,\n    reservation_name: str,\n    min_replica: int,\n    max_replica: int,\n    location: str,\n    deployed_endpoint: Output[VertexEndpoint],\n    deployed_model: Output[VertexModel],\n    endpoint_id: Output[str],\n) -> None:\n    from google_cloud_pipeline_components.types.artifact_types import (\n        VertexModel,\n    )\n    from google.cloud import aiplatform\n\n    aiplatform.init(\n        project=project_id,\n        location=location,\n    )\n\n    endpoint_fqn = endpoint.uri.split(\"v1/\")[1]\n    model_fqn = f\"projects/{project_id}/locations/{location}/models/{model}\"\n    vertex_endpoint = aiplatform.Endpoint(endpoint_fqn)\n    vertex_model = aiplatform.Model(model_name=model_fqn)\n\n    vertex_endpoint.deploy(\n        model=vertex_model,\n        deployed_model_display_name=deployed_name,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        reservation_affinity_type=\"SPECIFIC_RESERVATION\",\n        reservation_affinity_key=\"compute.googleapis.com/reservation-name\",\n        reservation_affinity_values=[\n            f\"projects/{project_id}/zones/{reservation_zone}/reservations/{reservation_name}\"\n        ],\n        min_replica_count=min_replica,\n        max_replica_count=max_replica,\n        sync=True,\n    )\n    # return types\n    deployed_endpoint.uri = endpoint.uri\n    deployed_model.uri = f\"https://{location}-aiplatform.googleapis.com/v1/{model_fqn}\"\n    endpoint_id = endpoint.uri.split('/')[-1]\n\n"
          ],
          "image": "python:3.11"
        }
      },
      "exec-dataproc-create-pyspark-batch": {
        "container": {
          "args": [
            "--type",
            "DataprocPySparkBatch",
            "--payload",
            "{\"Concat\": [\"{\", \"\\\"labels\\\": \", \"{{$.inputs.parameters['labels']}}\", \", \\\"runtime_config\\\": {\", \"\\\"version\\\": \\\"\", \"{{$.inputs.parameters['runtime_config_version']}}\", \"\\\"\", \", \\\"container_image\\\": \\\"\", \"{{$.inputs.parameters['container_image']}}\", \"\\\"\", \", \\\"properties\\\": \", \"{{$.inputs.parameters['runtime_config_properties']}}\", \"}\", \", \\\"environment_config\\\": {\", \"\\\"execution_config\\\": {\", \"\\\"service_account\\\": \\\"\", \"{{$.inputs.parameters['service_account']}}\", \"\\\"\", \", \\\"network_tags\\\": \", \"{{$.inputs.parameters['network_tags']}}\", \", \\\"kms_key\\\": \\\"\", \"{{$.inputs.parameters['kms_key']}}\", \"\\\"\", \", \\\"network_uri\\\": \\\"\", \"{{$.inputs.parameters['network_uri']}}\", \"\\\"\", \", \\\"subnetwork_uri\\\": \\\"\", \"{{$.inputs.parameters['subnetwork_uri']}}\", \"\\\"\", \"}\", \", \\\"peripherals_config\\\": {\", \"\\\"metastore_service\\\": \\\"\", \"{{$.inputs.parameters['metastore_service']}}\", \"\\\"\", \", \\\"spark_history_server_config\\\": { \", \"\\\"dataproc_cluster\\\": \\\"\", \"{{$.inputs.parameters['spark_history_dataproc_cluster']}}\", \"\\\"\", \"}\", \"}\", \"}\", \", \\\"pyspark_batch\\\": {\", \"\\\"main_python_file_uri\\\": \\\"\", \"{{$.inputs.parameters['main_python_file_uri']}}\", \"\\\"\", \", \\\"python_file_uris\\\": \", \"{{$.inputs.parameters['python_file_uris']}}\", \", \\\"jar_file_uris\\\": \", \"{{$.inputs.parameters['jar_file_uris']}}\", \", \\\"file_uris\\\": \", \"{{$.inputs.parameters['file_uris']}}\", \", \\\"archive_uris\\\": \", \"{{$.inputs.parameters['archive_uris']}}\", \", \\\"args\\\": \", \"{{$.inputs.parameters['args']}}\", \"}\", \"}\"]}",
            "--project",
            "{{$.inputs.parameters['project']}}",
            "--location",
            "{{$.inputs.parameters['location']}}",
            "--batch_id",
            "{{$.inputs.parameters['batch_id']}}",
            "--gcp_resources",
            "{{$.outputs.parameters['gcp_resources'].output_file}}"
          ],
          "command": [
            "python3",
            "-u",
            "-m",
            "google_cloud_pipeline_components.container.v1.dataproc.create_pyspark_batch.launcher"
          ],
          "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:2.20.0"
        }
      },
      "exec-endpoint-create": {
        "container": {
          "args": [
            "--type",
            "CreateEndpoint",
            "--payload",
            "{\"Concat\": [\"{\", \"\\\"display_name\\\": \\\"\", \"{{$.inputs.parameters['display_name']}}\", \"\\\"\", \", \\\"description\\\": \\\"\", \"{{$.inputs.parameters['description']}}\", \"\\\"\", \", \\\"labels\\\": \", \"{{$.inputs.parameters['labels']}}\", \", \\\"encryption_spec\\\": {\\\"kms_key_name\\\":\\\"\", \"{{$.inputs.parameters['encryption_spec_key_name']}}\", \"\\\"}\", \", \\\"network\\\": \\\"\", \"{{$.inputs.parameters['network']}}\", \"\\\"\", \"}\"]}",
            "--project",
            "{{$.inputs.parameters['project']}}",
            "--location",
            "{{$.inputs.parameters['location']}}",
            "--gcp_resources",
            "{{$.outputs.parameters['gcp_resources'].output_file}}",
            "--executor_input",
            "{{$}}"
          ],
          "command": [
            "python3",
            "-u",
            "-m",
            "google_cloud_pipeline_components.container.v1.endpoint.create_endpoint.launcher"
          ],
          "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:2.20.0"
        }
      },
      "exec-endpoint-delete": {
        "container": {
          "args": [
            "--type",
            "DeleteEndpoint",
            "--payload",
            "{\"Concat\": [\"{\", \"\\\"endpoint\\\": \\\"\", \"{{$.inputs.artifacts['endpoint'].metadata['resourceName']}}\", \"\\\"\", \"}\"]}",
            "--project",
            "",
            "--location",
            "",
            "--gcp_resources",
            "{{$.outputs.parameters['gcp_resources'].output_file}}"
          ],
          "command": [
            "python3",
            "-u",
            "-m",
            "google_cloud_pipeline_components.container.v1.endpoint.delete_endpoint.launcher"
          ],
          "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:2.20.0"
        }
      },
      "exec-model-undeploy": {
        "container": {
          "args": [
            "--type",
            "UndeployModel",
            "--payload",
            "{\"Concat\": [\"{\", \"\\\"endpoint\\\": \\\"\", \"{{$.inputs.artifacts['endpoint'].metadata['resourceName']}}\", \"\\\"\", \", \\\"model\\\": \\\"\", \"{{$.inputs.artifacts['model'].metadata['resourceName']}}\", \"\\\"\", \", \\\"traffic_split\\\": \", \"{{$.inputs.parameters['traffic_split']}}\", \"}\"]}",
            "--project",
            "",
            "--location",
            "",
            "--gcp_resources",
            "{{$.outputs.parameters['gcp_resources'].output_file}}"
          ],
          "command": [
            "python3",
            "-u",
            "-m",
            "google_cloud_pipeline_components.container.v1.endpoint.undeploy_model.launcher"
          ],
          "image": "gcr.io/ml-pipeline/google-cloud-pipeline-components:2.20.0"
        }
      }
    }
  },
  "pipelineInfo": {
    "description": "Deploys a model to an endpoint using a reserved GPU.",
    "name": "deploy-model-with-reserved-gpu"
  },
  "root": {
    "dag": {
      "tasks": {
        "create-endpoint-with-reservation": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-create-endpoint-with-reservation"
          },
          "dependentTasks": [
            "endpoint-create"
          ],
          "inputs": {
            "artifacts": {
              "endpoint": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "endpoint",
                  "producerTask": "endpoint-create"
                }
              }
            },
            "parameters": {
              "accelerator_count": {
                "runtimeValue": {
                  "constant": 1.0
                }
              },
              "accelerator_type": {
                "componentInputParameter": "accelerator_type"
              },
              "deployed_name": {
                "componentInputParameter": "deployed_model_display_name"
              },
              "location": {
                "componentInputParameter": "region"
              },
              "machine_type": {
                "componentInputParameter": "machine_type"
              },
              "max_replica": {
                "runtimeValue": {
                  "constant": 2.0
                }
              },
              "min_replica": {
                "runtimeValue": {
                  "constant": 1.0
                }
              },
              "model": {
                "componentInputParameter": "model"
              },
              "project_id": {
                "componentInputParameter": "shared_project_id"
              },
              "reservation_name": {
                "componentInputParameter": "reservation_name"
              },
              "reservation_zone": {
                "componentInputParameter": "zone"
              }
            }
          },
          "taskInfo": {
            "name": "create-endpoint-with-reservation"
          }
        },
        "dataproc-create-pyspark-batch": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-dataproc-create-pyspark-batch"
          },
          "dependentTasks": [
            "create-endpoint-with-reservation"
          ],
          "inputs": {
            "parameters": {
              "args": {
                "runtimeValue": {
                  "constant": [
                    "{{$.inputs.parameters['pipelinechannel--project_id']}}",
                    "{{$.inputs.parameters['pipelinechannel--region']}}",
                    "{{$.inputs.parameters['pipelinechannel--create-endpoint-with-reservation-endpoint_id']}}",
                    "{{$.inputs.parameters['pipelinechannel--bucket']}}",
                    "{{$.inputs.parameters['pipelinechannel--prediction_input_blob']}}",
                    "{{$.inputs.parameters['pipelinechannel--prediction_output_blob']}}",
                    30.0
                  ]
                }
              },
              "main_python_file_uri": {
                "runtimeValue": {
                  "constant": "gs://model_experimentation_2025/scripts/spark_batch_predict.py"
                }
              },
              "pipelinechannel--bucket": {
                "componentInputParameter": "bucket"
              },
              "pipelinechannel--create-endpoint-with-reservation-endpoint_id": {
                "taskOutputParameter": {
                  "outputParameterKey": "endpoint_id",
                  "producerTask": "create-endpoint-with-reservation"
                }
              },
              "pipelinechannel--prediction_input_blob": {
                "componentInputParameter": "prediction_input_blob"
              },
              "pipelinechannel--prediction_output_blob": {
                "componentInputParameter": "prediction_output_blob"
              },
              "pipelinechannel--project_id": {
                "componentInputParameter": "project_id"
              },
              "pipelinechannel--region": {
                "componentInputParameter": "region"
              }
            }
          },
          "taskInfo": {
            "name": "dataproc-create-pyspark-batch"
          }
        },
        "endpoint-create": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-endpoint-create"
          },
          "inputs": {
            "parameters": {
              "display_name": {
                "componentInputParameter": "endpoint_display_name"
              },
              "location": {
                "componentInputParameter": "region"
              },
              "project": {
                "componentInputParameter": "project_id"
              }
            }
          },
          "taskInfo": {
            "name": "endpoint-create"
          }
        },
        "endpoint-delete": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-endpoint-delete"
          },
          "dependentTasks": [
            "create-endpoint-with-reservation",
            "dataproc-create-pyspark-batch"
          ],
          "inputs": {
            "artifacts": {
              "endpoint": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "deployed_endpoint",
                  "producerTask": "create-endpoint-with-reservation"
                }
              }
            }
          },
          "taskInfo": {
            "name": "endpoint-delete"
          }
        },
        "model-undeploy": {
          "cachingOptions": {
            "enableCache": true
          },
          "componentRef": {
            "name": "comp-model-undeploy"
          },
          "dependentTasks": [
            "create-endpoint-with-reservation",
            "dataproc-create-pyspark-batch"
          ],
          "inputs": {
            "artifacts": {
              "endpoint": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "deployed_endpoint",
                  "producerTask": "create-endpoint-with-reservation"
                }
              },
              "model": {
                "taskOutputArtifact": {
                  "outputArtifactKey": "deployed_model",
                  "producerTask": "create-endpoint-with-reservation"
                }
              }
            }
          },
          "taskInfo": {
            "name": "model-undeploy"
          }
        }
      }
    },
    "inputDefinitions": {
      "parameters": {
        "accelerator_type": {
          "parameterType": "STRING"
        },
        "bucket": {
          "parameterType": "STRING"
        },
        "deployed_model_display_name": {
          "parameterType": "STRING"
        },
        "endpoint_display_name": {
          "parameterType": "STRING"
        },
        "machine_type": {
          "parameterType": "STRING"
        },
        "model": {
          "parameterType": "STRING"
        },
        "prediction_input_blob": {
          "parameterType": "STRING"
        },
        "prediction_output_blob": {
          "parameterType": "STRING"
        },
        "project_id": {
          "parameterType": "STRING"
        },
        "region": {
          "parameterType": "STRING"
        },
        "reservation_name": {
          "parameterType": "STRING"
        },
        "shared_project_id": {
          "parameterType": "STRING"
        },
        "zone": {
          "parameterType": "STRING"
        }
      }
    }
  },
  "schemaVersion": "2.1.0",
  "sdkVersion": "kfp-2.13.0"
}